{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc60c9ab",
   "metadata": {},
   "source": [
    "# Class For DecisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "73c2b7e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting full pipeline...\n",
      "\n",
      "Data loaded successfully: 68520 rows, 13 columns\n",
      "Data preprocessed: Train size = 54816, Test size = 13704\n",
      "Model trained successfully!\n",
      "\n",
      "==================================================\n",
      "MODEL PERFORMANCE METRICS\n",
      "==================================================\n",
      "Accuracy:  0.7211\n",
      "Precision: 0.7326\n",
      "Recall:    0.6865\n",
      "F1-Score:  0.7088\n",
      "\n",
      "Confusion Matrix:\n",
      "[[5231 1698]\n",
      " [2124 4651]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  No Disease       0.71      0.75      0.73      6929\n",
      "     Disease       0.73      0.69      0.71      6775\n",
      "\n",
      "    accuracy                           0.72     13704\n",
      "   macro avg       0.72      0.72      0.72     13704\n",
      "weighted avg       0.72      0.72      0.72     13704\n",
      "\n",
      "==================================================\n",
      "\n",
      "Predictions exported to: cardio_train_pred.csv\n",
      "Pipeline completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
    "                             f1_score, confusion_matrix, classification_report)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class CardioDecisionTreeClassifier:\n",
    "    \"\"\"\n",
    "    A reusable Decision Tree classifier for cardiovascular disease prediction.\n",
    "    \n",
    "    Features:\n",
    "    - Train/test split with customizable ratio\n",
    "    - Optional feature scaling\n",
    "    - Comprehensive performance metrics\n",
    "    - Export predictions to CSV\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, test_size=0.2, random_state=42, max_depth=None,\n",
    "                 min_samples_split=2, min_samples_leaf=1, scale_features=True,\n",
    "                 target_col='cardio', drop_cols=None):\n",
    "        \"\"\"\n",
    "        Initialize the classifier.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        test_size : float, default=0.2\n",
    "            Proportion of dataset to include in test split\n",
    "        random_state : int, default=42\n",
    "            Random state for reproducibility\n",
    "        max_depth : int, default=None\n",
    "            Maximum depth of the tree\n",
    "        min_samples_split : int, default=2\n",
    "            Minimum samples required to split a node\n",
    "        min_samples_leaf : int, default=1\n",
    "            Minimum samples required at a leaf node\n",
    "        scale_features : bool, default=True\n",
    "            Whether to scale features\n",
    "        target_col : str, default='cardio'\n",
    "            Name of the target variable column\n",
    "        drop_cols : list, default=None\n",
    "            List of columns to drop (e.g., ID columns)\n",
    "        \"\"\"\n",
    "        \n",
    "        self.test_size = test_size\n",
    "        self.random_state = random_state\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.scale_features = scale_features\n",
    "        self.target_col = target_col\n",
    "        self.drop_cols = drop_cols if drop_cols else []\n",
    "        \n",
    "        self.model = None\n",
    "        self.scaler = None\n",
    "        self.data = None\n",
    "        self.X_train = None\n",
    "        self.X_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "        self.feature_names = None\n",
    "        self.metrics = {}\n",
    "        \n",
    "    def load_data(self, filepath):\n",
    "        \"\"\"\n",
    "        Load data from CSV file.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        filepath : str\n",
    "            Path to the CSV file\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pd.DataFrame : Loaded dataframe\n",
    "        \"\"\"\n",
    "        self.data = pd.read_csv(filepath)\n",
    "        print(f\"Data loaded successfully: {self.data.shape[0]} rows, {self.data.shape[1]} columns\")\n",
    "        return self.data\n",
    "    \n",
    "    def preprocess_data(self):\n",
    "        \"\"\"\n",
    "        Preprocess the data for training.\n",
    "        Uses `self.target_col` and `self.drop_cols` from initialization.\n",
    "        \"\"\"\n",
    "        if self.data is None:\n",
    "            raise ValueError(\"No data loaded. Call load_data() first.\")\n",
    "        \n",
    "        df = self.data.copy()\n",
    "        if self.drop_cols:\n",
    "            df = df.drop(columns=self.drop_cols, errors='ignore')\n",
    "        \n",
    "        X = df.drop(columns=[self.target_col])\n",
    "        y = df[self.target_col]\n",
    "        \n",
    "        self.feature_names = X.columns.tolist()\n",
    "        \n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            X, y, test_size=self.test_size, random_state=self.random_state, stratify=y\n",
    "        )\n",
    "        \n",
    "        if self.scale_features:\n",
    "            self.scaler = StandardScaler()\n",
    "            self.X_train = pd.DataFrame(\n",
    "                self.scaler.fit_transform(self.X_train),\n",
    "                columns=self.feature_names,\n",
    "                index=self.X_train.index\n",
    "            )\n",
    "            self.X_test = pd.DataFrame(\n",
    "                self.scaler.transform(self.X_test),\n",
    "                columns=self.feature_names,\n",
    "                index=self.X_test.index\n",
    "            )\n",
    "        \n",
    "        print(f\"Data preprocessed: Train size = {len(self.X_train)}, Test size = {len(self.X_test)}\")\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Train the Decision Tree classifier.\"\"\"\n",
    "        if self.X_train is None:\n",
    "            raise ValueError(\"Data not preprocessed. Call preprocess_data() first.\")\n",
    "        \n",
    "        self.model = DecisionTreeClassifier(\n",
    "            max_depth=self.max_depth,\n",
    "            min_samples_split=self.min_samples_split,\n",
    "            min_samples_leaf=self.min_samples_leaf,\n",
    "            random_state=self.random_state\n",
    "        )\n",
    "        \n",
    "        self.model.fit(self.X_train, self.y_train)\n",
    "        print(\"Model trained successfully!\")\n",
    "    \n",
    "    def predict(self, X=None):\n",
    "        \"\"\"\n",
    "        Make predictions on data.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : pd.DataFrame, default=None\n",
    "            Data to predict. If None, uses test set.\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        np.ndarray : Predictions\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained. Call train() first.\")\n",
    "        \n",
    "        if X is None:\n",
    "            X_to_predict = self.X_test\n",
    "        else:\n",
    "            X_to_predict = X\n",
    "\n",
    "        if X is not None and self.scale_features and self.scaler is not None:\n",
    "            X_to_predict = pd.DataFrame(\n",
    "                self.scaler.transform(X_to_predict),\n",
    "                columns=X_to_predict.columns,\n",
    "                index=X_to_predict.index\n",
    "            )\n",
    "        \n",
    "        return self.model.predict(X_to_predict)\n",
    "    \n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        Evaluate model performance on test set.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict : Dictionary containing all performance metrics\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained. Call train() first.\")\n",
    "        \n",
    "        y_pred = self.predict()\n",
    "        \n",
    "        self.metrics = {\n",
    "            'accuracy': accuracy_score(self.y_test, y_pred),\n",
    "            'precision': precision_score(self.y_test, y_pred, average='binary'),\n",
    "            'recall': recall_score(self.y_test, y_pred, average='binary'),\n",
    "            'f1_score': f1_score(self.y_test, y_pred, average='binary'),\n",
    "            'confusion_matrix': confusion_matrix(self.y_test, y_pred)\n",
    "        }\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"MODEL PERFORMANCE METRICS\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"Accuracy:  {self.metrics['accuracy']:.4f}\")\n",
    "        print(f\"Precision: {self.metrics['precision']:.4f}\")\n",
    "        print(f\"Recall:    {self.metrics['recall']:.4f}\")\n",
    "        print(f\"F1-Score:  {self.metrics['f1_score']:.4f}\")\n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        print(self.metrics['confusion_matrix'])\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(self.y_test, y_pred, \n",
    "                                      target_names=['No Disease', 'Disease']))\n",
    "        print(\"=\"*50 + \"\\n\")\n",
    "        \n",
    "        return self.metrics\n",
    "    \n",
    "    def export_predictions(self, output_filepath, include_all_data=True):\n",
    "        \"\"\"\n",
    "        Export predictions for the *entire* loaded dataset to a new CSV file.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        output_filepath : str\n",
    "            Path for the output CSV file\n",
    "        include_all_data : bool, default=True\n",
    "            If True, includes all original columns. If False, only includes\n",
    "            the target and predicted columns.\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained. Call train() first.\")\n",
    "        if self.data is None:\n",
    "            raise ValueError(\"No data loaded. Call load_data() first.\")\n",
    "            \n",
    "        \n",
    "        # Prepare the full dataset for prediction\n",
    "        X_full = self.data.copy()\n",
    "        \n",
    "        if self.drop_cols:\n",
    "            X_full = X_full.drop(columns=self.drop_cols, errors='ignore')\n",
    "            \n",
    "        X_full = X_full.drop(columns=[self.target_col], errors='ignore')\n",
    "        \n",
    "        # Ensure columns match training data\n",
    "        X_full = X_full[self.feature_names]\n",
    "        \n",
    "        if self.scale_features and self.scaler is not None:\n",
    "            X_full_scaled = pd.DataFrame(\n",
    "                self.scaler.transform(X_full),\n",
    "                columns=self.feature_names,\n",
    "                index=X_full.index\n",
    "            )\n",
    "            predictions = self.model.predict(X_full_scaled)\n",
    "        else:\n",
    "            predictions = self.model.predict(X_full)\n",
    "        \n",
    "        if include_all_data:\n",
    "            output_df = self.data.copy()\n",
    "        else:\n",
    "            output_df = pd.DataFrame()\n",
    "            output_df[self.target_col] = self.data[self.target_col]\n",
    "        \n",
    "        output_df['predicted'] = predictions\n",
    "        \n",
    "        output_df.to_csv(output_filepath, index=False)\n",
    "        print(f\"Predictions exported to: {output_filepath}\")\n",
    "        \n",
    "        return output_df\n",
    "    \n",
    "    def run_full_pipeline(self, input_filepath, output_filepath, include_all_data=True):\n",
    "        \"\"\"\n",
    "        Run the complete pipeline: load, preprocess, train, evaluate, and export.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        input_filepath : str\n",
    "            Path to input CSV file\n",
    "        output_filepath : str\n",
    "            Path for output CSV file with predictions\n",
    "        include_all_data : bool, default=True\n",
    "            Passed to export_predictions to control output file content.\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict : Performance metrics\n",
    "        \"\"\"\n",
    "        print(\"Starting full pipeline...\\n\")\n",
    "        \n",
    "        self.load_data(input_filepath)\n",
    "        self.preprocess_data()\n",
    "        self.train()\n",
    "        metrics = self.evaluate()\n",
    "        self.export_predictions(output_filepath, include_all_data=include_all_data)\n",
    "        \n",
    "        print(\"Pipeline completed successfully!\")\n",
    "        return metrics\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Configure the model with all parameters during initialization\n",
    "    clf = CardioDecisionTreeClassifier(\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        max_depth=10,\n",
    "        min_samples_split=20,\n",
    "        min_samples_leaf=10,\n",
    "        scale_features=True,\n",
    "        target_col='cardio',  # <--- Parameter moved here\n",
    "        drop_cols=['id']      # <--- Parameter moved here\n",
    "    )\n",
    "    \n",
    "    # Run the pipeline with just the filepaths\n",
    "    metrics = clf.run_full_pipeline(\n",
    "        input_filepath='train.csv',\n",
    "        output_filepath='cardio_train_pred.csv'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961494d3",
   "metadata": {},
   "source": [
    "# Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "017a75b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting full pipeline...\n",
      "\n",
      "Data loaded successfully: 70000 rows, 13 columns\n",
      "Data preprocessed: Train size = 49000, Test size = 21000\n",
      "Model trained successfully!\n",
      "\n",
      "==================================================\n",
      "MODEL PERFORMANCE METRICS\n",
      "==================================================\n",
      "Accuracy:  0.7147\n",
      "Precision: 0.7348\n",
      "Recall:    0.6714\n",
      "F1-Score:  0.7017\n",
      "\n",
      "Confusion Matrix:\n",
      "[[7963 2543]\n",
      " [3448 7046]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  No Disease       0.70      0.76      0.73     10506\n",
      "     Disease       0.73      0.67      0.70     10494\n",
      "\n",
      "    accuracy                           0.71     21000\n",
      "   macro avg       0.72      0.71      0.71     21000\n",
      "weighted avg       0.72      0.71      0.71     21000\n",
      "\n",
      "==================================================\n",
      "\n",
      "Predictions exported to: cardio_train_pred_lr.csv\n",
      "Pipeline completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                             f1_score, confusion_matrix, classification_report)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class CardioLogisticRegressionClassifier:\n",
    "    \"\"\"\n",
    "    A reusable Logistic Regression classifier for cardiovascular disease prediction.\n",
    "    \n",
    "    Features:\n",
    "    - Train/test split with customizable ratio\n",
    "    - Optional feature scaling\n",
    "    - Comprehensive performance metrics\n",
    "    - Export predictions to CSV\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, test_size=0.2, random_state=42, C=1.0, \n",
    "                 solver='lbfgs', max_iter=100, scale_features=True,\n",
    "                 target_col='cardio', drop_cols=None):\n",
    "        \"\"\"\n",
    "        Initialize the classifier.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        test_size : float, default=0.2\n",
    "            Proportion of dataset to include in test split\n",
    "        random_state : int, default=42\n",
    "            Random state for reproducibility\n",
    "        C : float, default=1.0\n",
    "            Inverse of regularization strength; smaller values specify stronger regularization\n",
    "        solver : str, default='lbfgs'\n",
    "            Algorithm to use in the optimization problem\n",
    "        max_iter : int, default=100\n",
    "            Maximum number of iterations taken for the solvers to converge\n",
    "        scale_features : bool, default=True\n",
    "            Whether to scale features\n",
    "        target_col : str, default='cardio'\n",
    "            Name of the target variable column\n",
    "        drop_cols : list, default=None\n",
    "            List of columns to drop (e.g., ID columns)\n",
    "        \"\"\"\n",
    "\n",
    "        self.test_size = test_size\n",
    "        self.random_state = random_state\n",
    "        self.C = C\n",
    "        self.solver = solver\n",
    "        self.max_iter = max_iter\n",
    "        self.scale_features = scale_features\n",
    "        self.target_col = target_col\n",
    "        self.drop_cols = drop_cols if drop_cols else []\n",
    "        \n",
    "        self.model = None\n",
    "        self.scaler = None\n",
    "        self.data = None\n",
    "        self.X_train = None\n",
    "        self.X_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "        self.feature_names = None\n",
    "        self.metrics = {}\n",
    "        \n",
    "    def load_data(self, filepath):\n",
    "        \"\"\"\n",
    "        Load data from CSV file.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        filepath : str\n",
    "            Path to the CSV file\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pd.DataFrame : Loaded dataframe\n",
    "        \"\"\"\n",
    "        self.data = pd.read_csv(filepath)\n",
    "        print(f\"Data loaded successfully: {self.data.shape[0]} rows, {self.data.shape[1]} columns\")\n",
    "        return self.data\n",
    "    \n",
    "    def preprocess_data(self):\n",
    "        \"\"\"\n",
    "        Preprocess the data for training.\n",
    "        Uses `self.target_col` and `self.drop_cols` from initialization.\n",
    "        \"\"\"\n",
    "        if self.data is None:\n",
    "            raise ValueError(\"No data loaded. Call load_data() first.\")\n",
    "        \n",
    "        df = self.data.copy()\n",
    "        if self.drop_cols:\n",
    "            df = df.drop(columns=self.drop_cols, errors='ignore')\n",
    "        \n",
    "        X = df.drop(columns=[self.target_col])\n",
    "        y = df[self.target_col]\n",
    "        \n",
    "        self.feature_names = X.columns.tolist()\n",
    "        \n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            X, y, test_size=self.test_size, random_state=self.random_state, stratify=y\n",
    "        )\n",
    "        \n",
    "        if self.scale_features:\n",
    "            self.scaler = StandardScaler()\n",
    "            self.X_train = pd.DataFrame(\n",
    "                self.scaler.fit_transform(self.X_train),\n",
    "                columns=self.feature_names,\n",
    "                index=self.X_train.index\n",
    "            )\n",
    "            self.X_test = pd.DataFrame(\n",
    "                self.scaler.transform(self.X_test),\n",
    "                columns=self.feature_names,\n",
    "                index=self.X_test.index\n",
    "            )\n",
    "        \n",
    "        print(f\"Data preprocessed: Train size = {len(self.X_train)}, Test size = {len(self.X_test)}\")\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Train the Logistic Regression classifier.\"\"\"\n",
    "        if self.X_train is None:\n",
    "            raise ValueError(\"Data not preprocessed. Call preprocess_data() first.\")\n",
    "        \n",
    "        self.model = LogisticRegression(\n",
    "            C=self.C,\n",
    "            solver=self.solver,\n",
    "            max_iter=self.max_iter,\n",
    "            random_state=self.random_state\n",
    "        )\n",
    "        \n",
    "        self.model.fit(self.X_train, self.y_train)\n",
    "        print(\"Model trained successfully!\")\n",
    "    \n",
    "    def predict(self, X=None):\n",
    "        \"\"\"\n",
    "        Make predictions on data.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : pd.DataFrame, default=None\n",
    "            Data to predict. If None, uses test set.\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        np.ndarray : Predictions\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained. Call train() first.\")\n",
    "        \n",
    "        if X is None:\n",
    "            X_to_predict = self.X_test\n",
    "        else:\n",
    "            X_to_predict = X\n",
    "\n",
    "        if X is not None and self.scale_features and self.scaler is not None:\n",
    "            # When predicting on new, external data\n",
    "            X_to_predict = pd.DataFrame(\n",
    "                self.scaler.transform(X_to_predict),\n",
    "                columns=X_to_predict.columns,\n",
    "                index=X_to_predict.index\n",
    "            )\n",
    "        \n",
    "        return self.model.predict(X_to_predict)\n",
    "    \n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        Evaluate model performance on test set.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict : Dictionary containing all performance metrics\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained. Call train() first.\")\n",
    "        \n",
    "        y_pred = self.predict()\n",
    "        \n",
    "        self.metrics = {\n",
    "            'accuracy': accuracy_score(self.y_test, y_pred),\n",
    "            'precision': precision_score(self.y_test, y_pred, average='binary'),\n",
    "            'recall': recall_score(self.y_test, y_pred, average='binary'),\n",
    "            'f1_score': f1_score(self.y_test, y_pred, average='binary'),\n",
    "            'confusion_matrix': confusion_matrix(self.y_test, y_pred)\n",
    "        }\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"MODEL PERFORMANCE METRICS\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"Accuracy:  {self.metrics['accuracy']:.4f}\")\n",
    "        print(f\"Precision: {self.metrics['precision']:.4f}\")\n",
    "        print(f\"Recall:    {self.metrics['recall']:.4f}\")\n",
    "        print(f\"F1-Score:  {self.metrics['f1_score']:.4f}\")\n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        print(self.metrics['confusion_matrix'])\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(self.y_test, y_pred, \n",
    "                                      target_names=['No Disease', 'Disease']))\n",
    "        print(\"=\"*50 + \"\\n\")\n",
    "        \n",
    "        return self.metrics\n",
    "    \n",
    "    def export_predictions(self, output_filepath, include_all_data=True):\n",
    "        \"\"\"\n",
    "        Export predictions for the *entire* loaded dataset to a new CSV file.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        output_filepath : str\n",
    "            Path for the output CSV file\n",
    "        include_all_data : bool, default=True\n",
    "            If True, includes all original columns. If False, only includes\n",
    "            the target and predicted columns.\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained. Call train() first.\")\n",
    "        if self.data is None:\n",
    "            raise ValueError(\"No data loaded. Call load_data() first.\")\n",
    "            \n",
    "        \n",
    "        # Prepare the full dataset for prediction\n",
    "        X_full = self.data.copy()\n",
    "        \n",
    "        if self.drop_cols:\n",
    "            X_full = X_full.drop(columns=self.drop_cols, errors='ignore')\n",
    "            \n",
    "        X_full = X_full.drop(columns=[self.target_col], errors='ignore')\n",
    "        \n",
    "        # Ensure columns match training data\n",
    "        X_full = X_full[self.feature_names]\n",
    "        \n",
    "        if self.scale_features and self.scaler is not None:\n",
    "            X_full_scaled = pd.DataFrame(\n",
    "                self.scaler.transform(X_full),\n",
    "                columns=self.feature_names,\n",
    "                index=X_full.index\n",
    "            )\n",
    "            predictions = self.model.predict(X_full_scaled)\n",
    "        else:\n",
    "            predictions = self.model.predict(X_full)\n",
    "        \n",
    "        if include_all_data:\n",
    "            output_df = self.data.copy()\n",
    "        else:\n",
    "            output_df = pd.DataFrame()\n",
    "            output_df[self.target_col] = self.data[self.target_col]\n",
    "        \n",
    "        output_df['predicted'] = predictions\n",
    "        \n",
    "        output_df.to_csv(output_filepath, index=False)\n",
    "        print(f\"Predictions exported to: {output_filepath}\")\n",
    "        \n",
    "        return output_df\n",
    "    \n",
    "    def run_full_pipeline(self, input_filepath, output_filepath, include_all_data=True):\n",
    "        \"\"\"\n",
    "        Run the complete pipeline: load, preprocess, train, evaluate, and export.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        input_filepath : str\n",
    "            Path to input CSV file\n",
    "        output_filepath : str\n",
    "            Path for output CSV file with predictions\n",
    "        include_all_data : bool, default=True\n",
    "            Passed to export_predictions to control output file content.\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict : Performance metrics\n",
    "        \"\"\"\n",
    "        print(\"Starting full pipeline...\\n\")\n",
    "        \n",
    "        self.load_data(input_filepath)\n",
    "        self.preprocess_data()\n",
    "        self.train()\n",
    "        metrics = self.evaluate()\n",
    "        self.export_predictions(output_filepath, include_all_data=include_all_data)\n",
    "        \n",
    "        print(\"Pipeline completed successfully!\")\n",
    "        return metrics\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Configure the model with all parameters during initialization\n",
    "    clf = CardioLogisticRegressionClassifier(\n",
    "        test_size=0.3,\n",
    "        random_state=12,\n",
    "        C=1.0,\n",
    "        solver='lbfgs',\n",
    "        max_iter=10000, # Increased max_iter for convergence\n",
    "        scale_features=True,\n",
    "        target_col='cardio',  # <--- Parameter moved here\n",
    "        drop_cols=['id']      # <--- Parameter moved here\n",
    "    )\n",
    "    \n",
    "    # Run the pipeline with just the filepaths\n",
    "    metrics = clf.run_full_pipeline(\n",
    "        input_filepath='cardio_train.csv',\n",
    "        output_filepath='cardio_train_pred_lr.csv' # Changed output name\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20174f7b",
   "metadata": {},
   "source": [
    "# Random Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "45588fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting full pipeline...\n",
      "\n",
      "Data loaded successfully: 70000 rows, 13 columns\n",
      "Data preprocessed: Train size = 56000, Test size = 14000\n",
      "Model trained successfully!\n",
      "\n",
      "==================================================\n",
      "MODEL PERFORMANCE METRICS\n",
      "==================================================\n",
      "Accuracy:  0.7313\n",
      "Precision: 0.7492\n",
      "Recall:    0.6950\n",
      "F1-Score:  0.7210\n",
      "\n",
      "Confusion Matrix:\n",
      "[[5376 1628]\n",
      " [2134 4862]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  No Disease       0.72      0.77      0.74      7004\n",
      "     Disease       0.75      0.69      0.72      6996\n",
      "\n",
      "    accuracy                           0.73     14000\n",
      "   macro avg       0.73      0.73      0.73     14000\n",
      "weighted avg       0.73      0.73      0.73     14000\n",
      "\n",
      "==================================================\n",
      "\n",
      "Predictions exported to: cardio_train_pred_rf.csv\n",
      "Pipeline completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                             f1_score, confusion_matrix, classification_report)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class CardioRandomForestClassifier:\n",
    "    \"\"\"\n",
    "    A reusable Random Forest classifier for cardiovascular disease prediction.\n",
    "    \n",
    "    Features:\n",
    "    - Train/test split with customizable ratio\n",
    "    - Optional feature scaling\n",
    "    - Comprehensive performance metrics\n",
    "    - Export predictions to CSV\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, test_size=0.2, random_state=42, n_estimators=100, \n",
    "                 max_depth=None, min_samples_split=2, min_samples_leaf=1, \n",
    "                 scale_features=True, target_col='cardio', drop_cols=None):\n",
    "        \"\"\"\n",
    "        Initialize the classifier.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        test_size : float, default=0.2\n",
    "            Proportion of dataset to include in test split\n",
    "        random_state : int, default=42\n",
    "            Random state for reproducibility\n",
    "        n_estimators : int, default=100\n",
    "            Number of trees in the forest\n",
    "        max_depth : int, default=None\n",
    "            Maximum depth of the tree\n",
    "        min_samples_split : int, default=2\n",
    "            Minimum samples required to split a node\n",
    "        min_samples_leaf : int, default=1\n",
    "            Minimum samples required at a leaf node\n",
    "        scale_features : bool, default=True\n",
    "            Whether to scale features\n",
    "        target_col : str, default='cardio'\n",
    "            Name of the target variable column\n",
    "        drop_cols : list, default=None\n",
    "            List of columns to drop (e.g., ID columns)\n",
    "        \"\"\"\n",
    "\n",
    "        self.test_size = test_size\n",
    "        self.random_state = random_state\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.scale_features = scale_features\n",
    "        self.target_col = target_col\n",
    "        self.drop_cols = drop_cols if drop_cols else []\n",
    "        \n",
    "        self.model = None\n",
    "        self.scaler = None\n",
    "        self.data = None\n",
    "        self.X_train = None\n",
    "        self.X_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "        self.feature_names = None\n",
    "        self.metrics = {}\n",
    "        \n",
    "    def load_data(self, filepath):\n",
    "        \"\"\"\n",
    "        Load data from CSV file.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        filepath : str\n",
    "            Path to the CSV file\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pd.DataFrame : Loaded dataframe\n",
    "        \"\"\"\n",
    "        self.data = pd.read_csv(filepath)\n",
    "        print(f\"Data loaded successfully: {self.data.shape[0]} rows, {self.data.shape[1]} columns\")\n",
    "        return self.data\n",
    "    \n",
    "    def preprocess_data(self):\n",
    "        \"\"\"\n",
    "        Preprocess the data for training.\n",
    "        Uses `self.target_col` and `self.drop_cols` from initialization.\n",
    "        \"\"\"\n",
    "        if self.data is None:\n",
    "            raise ValueError(\"No data loaded. Call load_data() first.\")\n",
    "        \n",
    "        df = self.data.copy()\n",
    "        if self.drop_cols:\n",
    "            df = df.drop(columns=self.drop_cols, errors='ignore')\n",
    "        \n",
    "        X = df.drop(columns=[self.target_col])\n",
    "        y = df[self.target_col]\n",
    "        \n",
    "        self.feature_names = X.columns.tolist()\n",
    "        \n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            X, y, test_size=self.test_size, random_state=self.random_state, stratify=y\n",
    "        )\n",
    "        \n",
    "        if self.scale_features:\n",
    "            self.scaler = StandardScaler()\n",
    "            self.X_train = pd.DataFrame(\n",
    "                self.scaler.fit_transform(self.X_train),\n",
    "                columns=self.feature_names,\n",
    "                index=self.X_train.index\n",
    "            )\n",
    "            self.X_test = pd.DataFrame(\n",
    "                self.scaler.transform(self.X_test),\n",
    "                columns=self.feature_names,\n",
    "                index=self.X_test.index\n",
    "            )\n",
    "        \n",
    "        print(f\"Data preprocessed: Train size = {len(self.X_train)}, Test size = {len(self.X_test)}\")\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Train the Random Forest classifier.\"\"\"\n",
    "        if self.X_train is None:\n",
    "            raise ValueError(\"Data not preprocessed. Call preprocess_data() first.\")\n",
    "        \n",
    "        self.model = RandomForestClassifier(\n",
    "            n_estimators=self.n_estimators,\n",
    "            max_depth=self.max_depth,\n",
    "            min_samples_split=self.min_samples_split,\n",
    "            min_samples_leaf=self.min_samples_leaf,\n",
    "            random_state=self.random_state\n",
    "        )\n",
    "        \n",
    "        self.model.fit(self.X_train, self.y_train)\n",
    "        print(\"Model trained successfully!\")\n",
    "    \n",
    "    def predict(self, X=None):\n",
    "        \"\"\"\n",
    "        Make predictions on data.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : pd.DataFrame, default=None\n",
    "            Data to predict. If None, uses test set.\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        np.ndarray : Predictions\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained. Call train() first.\")\n",
    "        \n",
    "        if X is None:\n",
    "            X_to_predict = self.X_test\n",
    "        else:\n",
    "            X_to_predict = X\n",
    "\n",
    "        if X is not None and self.scale_features and self.scaler is not None:\n",
    "            # When predicting on new, external data\n",
    "            X_to_predict = pd.DataFrame(\n",
    "                self.scaler.transform(X_to_predict),\n",
    "                columns=X_to_predict.columns,\n",
    "                index=X_to_predict.index\n",
    "            )\n",
    "        \n",
    "        return self.model.predict(X_to_predict)\n",
    "    \n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        Evaluate model performance on test set.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict : Dictionary containing all performance metrics\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained. Call train() first.\")\n",
    "        \n",
    "        y_pred = self.predict()\n",
    "        \n",
    "        self.metrics = {\n",
    "            'accuracy': accuracy_score(self.y_test, y_pred),\n",
    "            'precision': precision_score(self.y_test, y_pred, average='binary'),\n",
    "            'recall': recall_score(self.y_test, y_pred, average='binary'),\n",
    "            'f1_score': f1_score(self.y_test, y_pred, average='binary'),\n",
    "            'confusion_matrix': confusion_matrix(self.y_test, y_pred)\n",
    "        }\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"MODEL PERFORMANCE METRICS\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"Accuracy:  {self.metrics['accuracy']:.4f}\")\n",
    "        print(f\"Precision: {self.metrics['precision']:.4f}\")\n",
    "        print(f\"Recall:    {self.metrics['recall']:.4f}\")\n",
    "        print(f\"F1-Score:  {self.metrics['f1_score']:.4f}\")\n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        print(self.metrics['confusion_matrix'])\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(self.y_test, y_pred, \n",
    "                                      target_names=['No Disease', 'Disease']))\n",
    "        print(\"=\"*50 + \"\\n\")\n",
    "        \n",
    "        return self.metrics\n",
    "    \n",
    "    def export_predictions(self, output_filepath, include_all_data=True):\n",
    "        \"\"\"\n",
    "        Export predictions for the *entire* loaded dataset to a new CSV file.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        output_filepath : str\n",
    "            Path for the output CSV file\n",
    "        include_all_data : bool, default=True\n",
    "            If True, includes all original columns. If False, only includes\n",
    "            the target and predicted columns.\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained. Call train() first.\")\n",
    "        if self.data is None:\n",
    "            raise ValueError(\"No data loaded. Call load_data() first.\")\n",
    "            \n",
    "        \n",
    "        # Prepare the full dataset for prediction\n",
    "        X_full = self.data.copy()\n",
    "        \n",
    "        if self.drop_cols:\n",
    "            X_full = X_full.drop(columns=self.drop_cols, errors='ignore')\n",
    "            \n",
    "        X_full = X_full.drop(columns=[self.target_col], errors='ignore')\n",
    "        \n",
    "        # Ensure columns match training data\n",
    "        X_full = X_full[self.feature_names]\n",
    "        \n",
    "        if self.scale_features and self.scaler is not None:\n",
    "            X_full_scaled = pd.DataFrame(\n",
    "                self.scaler.transform(X_full),\n",
    "                columns=self.feature_names,\n",
    "                index=X_full.index\n",
    "            )\n",
    "            predictions = self.model.predict(X_full_scaled)\n",
    "        else:\n",
    "            predictions = self.model.predict(X_full)\n",
    "        \n",
    "        if include_all_data:\n",
    "            output_df = self.data.copy()\n",
    "        else:\n",
    "            output_df = pd.DataFrame()\n",
    "            output_df[self.target_col] = self.data[self.target_col]\n",
    "        \n",
    "        output_df['predicted'] = predictions\n",
    "        \n",
    "        output_df.to_csv(output_filepath, index=False)\n",
    "        print(f\"Predictions exported to: {output_filepath}\")\n",
    "        \n",
    "        return output_df\n",
    "    \n",
    "    def run_full_pipeline(self, input_filepath, output_filepath, include_all_data=True):\n",
    "        \"\"\"\n",
    "        Run the complete pipeline: load, preprocess, train, evaluate, and export.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        input_filepath : str\n",
    "            Path to input CSV file\n",
    "        output_filepath : str\n",
    "            Path for output CSV file with predictions\n",
    "        include_all_data : bool, default=True\n",
    "            Passed to export_predictions to control output file content.\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict : Performance metrics\n",
    "        \"\"\"\n",
    "        print(\"Starting full pipeline...\\n\")\n",
    "        \n",
    "        self.load_data(input_filepath)\n",
    "        self.preprocess_data()\n",
    "        self.train()\n",
    "        metrics = self.evaluate()\n",
    "        self.export_predictions(output_filepath, include_all_data=include_all_data)\n",
    "        \n",
    "        print(\"Pipeline completed successfully!\")\n",
    "        return metrics\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Configure the model with all parameters during initialization\n",
    "    clf = CardioRandomForestClassifier(\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        n_estimators=100,\n",
    "        max_depth=1000,\n",
    "        min_samples_split=20,\n",
    "        min_samples_leaf=10,\n",
    "        scale_features=True,\n",
    "        target_col='cardio',  # <--- Parameter moved here\n",
    "        drop_cols=['id']      # <--- Parameter moved here\n",
    "    )\n",
    "    \n",
    "    # Run the pipeline with just the filepaths\n",
    "    metrics = clf.run_full_pipeline(\n",
    "        input_filepath='cardio_train.csv',\n",
    "        output_filepath='cardio_train_pred_rf.csv' # Changed output name\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84754322",
   "metadata": {},
   "source": [
    "# Naive Base\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "573781eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting full pipeline...\n",
      "\n",
      "Data loaded successfully: 68520 rows, 13 columns\n",
      "Data preprocessed: Train size = 54816, Test size = 13704\n",
      "Model trained successfully!\n",
      "\n",
      "==================================================\n",
      "MODEL PERFORMANCE METRICS\n",
      "==================================================\n",
      "Accuracy:  0.7072\n",
      "Precision: 0.7501\n",
      "Recall:    0.6114\n",
      "F1-Score:  0.6737\n",
      "\n",
      "Confusion Matrix:\n",
      "[[5549 1380]\n",
      " [2633 4142]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  No Disease       0.68      0.80      0.73      6929\n",
      "     Disease       0.75      0.61      0.67      6775\n",
      "\n",
      "    accuracy                           0.71     13704\n",
      "   macro avg       0.71      0.71      0.70     13704\n",
      "weighted avg       0.71      0.71      0.70     13704\n",
      "\n",
      "==================================================\n",
      "\n",
      "Predictions exported to: cardio_train_pred_nb.csv\n",
      "Pipeline completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                             f1_score, confusion_matrix, classification_report)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class CardioNaiveBayesClassifier:\n",
    "    \"\"\"\n",
    "    A reusable Naive Bayes classifier for cardiovascular disease prediction.\n",
    "    \n",
    "    Features:\n",
    "    - Train/test split with customizable ratio\n",
    "    - Optional feature scaling\n",
    "    - Comprehensive performance metrics\n",
    "    - Export predictions to CSV\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, test_size=0.2, random_state=42, var_smoothing=1e-9, \n",
    "                 scale_features=True, target_col='cardio', drop_cols=None):\n",
    "        \"\"\"\n",
    "        Initialize the classifier.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        test_size : float, default=0.2\n",
    "            Proportion of dataset to include in test split\n",
    "        random_state : int, default=42\n",
    "            Random state for reproducibility\n",
    "        var_smoothing : float, default=1e-9\n",
    "            Portion of the largest variance of all features that is added to variances for calculation stability\n",
    "        scale_features : bool, default=True\n",
    "            Whether to scale features\n",
    "        target_col : str, default='cardio'\n",
    "            Name of the target variable column\n",
    "        drop_cols : list, default=None\n",
    "            List of columns to drop (e.g., ID columns)\n",
    "        \"\"\"\n",
    "\n",
    "        self.test_size = test_size\n",
    "        self.random_state = random_state\n",
    "        self.var_smoothing = var_smoothing\n",
    "        self.scale_features = scale_features\n",
    "        self.target_col = target_col\n",
    "        self.drop_cols = drop_cols if drop_cols else []\n",
    "        \n",
    "        self.model = None\n",
    "        self.scaler = None\n",
    "        self.data = None\n",
    "        self.X_train = None\n",
    "        self.X_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "        self.feature_names = None\n",
    "        self.metrics = {}\n",
    "        \n",
    "    def load_data(self, filepath):\n",
    "        \"\"\"\n",
    "        Load data from CSV file.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        filepath : str\n",
    "            Path to the CSV file\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pd.DataFrame : Loaded dataframe\n",
    "        \"\"\"\n",
    "        self.data = pd.read_csv(filepath)\n",
    "        print(f\"Data loaded successfully: {self.data.shape[0]} rows, {self.data.shape[1]} columns\")\n",
    "        return self.data\n",
    "    \n",
    "    def preprocess_data(self):\n",
    "        \"\"\"\n",
    "        Preprocess the data for training.\n",
    "        Uses `self.target_col` and `self.drop_cols` from initialization.\n",
    "        \"\"\"\n",
    "        if self.data is None:\n",
    "            raise ValueError(\"No data loaded. Call load_data() first.\")\n",
    "        \n",
    "        df = self.data.copy()\n",
    "        if self.drop_cols:\n",
    "            df = df.drop(columns=self.drop_cols, errors='ignore')\n",
    "        \n",
    "        X = df.drop(columns=[self.target_col])\n",
    "        y = df[self.target_col]\n",
    "        \n",
    "        self.feature_names = X.columns.tolist()\n",
    "        \n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            X, y, test_size=self.test_size, random_state=self.random_state, stratify=y\n",
    "        )\n",
    "        \n",
    "        if self.scale_features:\n",
    "            self.scaler = StandardScaler()\n",
    "            self.X_train = pd.DataFrame(\n",
    "                self.scaler.fit_transform(self.X_train),\n",
    "                columns=self.feature_names,\n",
    "                index=self.X_train.index\n",
    "            )\n",
    "            self.X_test = pd.DataFrame(\n",
    "                self.scaler.transform(self.X_test),\n",
    "                columns=self.feature_names,\n",
    "                index=self.X_test.index\n",
    "            )\n",
    "        \n",
    "        print(f\"Data preprocessed: Train size = {len(self.X_train)}, Test size = {len(self.X_test)}\")\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Train the Naive Bayes classifier.\"\"\"\n",
    "        if self.X_train is None:\n",
    "            raise ValueError(\"Data not preprocessed. Call preprocess_data() first.\")\n",
    "        \n",
    "        self.model = GaussianNB(\n",
    "            var_smoothing=self.var_smoothing\n",
    "        )\n",
    "        \n",
    "        self.model.fit(self.X_train, self.y_train)\n",
    "        print(\"Model trained successfully!\")\n",
    "    \n",
    "    def predict(self, X=None):\n",
    "        \"\"\"\n",
    "        Make predictions on data.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : pd.DataFrame, default=None\n",
    "            Data to predict. If None, uses test set.\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        np.ndarray : Predictions\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained. Call train() first.\")\n",
    "        \n",
    "        if X is None:\n",
    "            X_to_predict = self.X_test\n",
    "        else:\n",
    "            X_to_predict = X\n",
    "\n",
    "        if X is not None and self.scale_features and self.scaler is not None:\n",
    "            # When predicting on new, external data\n",
    "            X_to_predict = pd.DataFrame(\n",
    "                self.scaler.transform(X_to_predict),\n",
    "                columns=X_to_predict.columns,\n",
    "                index=X_to_predict.index\n",
    "            )\n",
    "        \n",
    "        return self.model.predict(X_to_predict)\n",
    "    \n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        Evaluate model performance on test set.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict : Dictionary containing all performance metrics\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained. Call train() first.\")\n",
    "        \n",
    "        y_pred = self.predict()\n",
    "        \n",
    "        self.metrics = {\n",
    "            'accuracy': accuracy_score(self.y_test, y_pred),\n",
    "            'precision': precision_score(self.y_test, y_pred, average='binary'),\n",
    "            'recall': recall_score(self.y_test, y_pred, average='binary'),\n",
    "            'f1_score': f1_score(self.y_test, y_pred, average='binary'),\n",
    "            'confusion_matrix': confusion_matrix(self.y_test, y_pred)\n",
    "        }\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"MODEL PERFORMANCE METRICS\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"Accuracy:  {self.metrics['accuracy']:.4f}\")\n",
    "        print(f\"Precision: {self.metrics['precision']:.4f}\")\n",
    "        print(f\"Recall:    {self.metrics['recall']:.4f}\")\n",
    "        print(f\"F1-Score:  {self.metrics['f1_score']:.4f}\")\n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        print(self.metrics['confusion_matrix'])\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(self.y_test, y_pred, \n",
    "                                      target_names=['No Disease', 'Disease']))\n",
    "        print(\"=\"*50 + \"\\n\")\n",
    "        \n",
    "        return self.metrics\n",
    "    \n",
    "    def export_predictions(self, output_filepath, include_all_data=True):\n",
    "        \"\"\"\n",
    "        Export predictions for the *entire* loaded dataset to a new CSV file.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        output_filepath : str\n",
    "            Path for the output CSV file\n",
    "        include_all_data : bool, default=True\n",
    "            If True, includes all original columns. If False, only includes\n",
    "            the target and predicted columns.\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained. Call train() first.\")\n",
    "        if self.data is None:\n",
    "            raise ValueError(\"No data loaded. Call load_data() first.\")\n",
    "            \n",
    "        \n",
    "        # Prepare the full dataset for prediction\n",
    "        X_full = self.data.copy()\n",
    "        \n",
    "        if self.drop_cols:\n",
    "            X_full = X_full.drop(columns=self.drop_cols, errors='ignore')\n",
    "            \n",
    "        X_full = X_full.drop(columns=[self.target_col], errors='ignore')\n",
    "        \n",
    "        # Ensure columns match training data\n",
    "        X_full = X_full[self.feature_names]\n",
    "        \n",
    "        if self.scale_features and self.scaler is not None:\n",
    "            X_full_scaled = pd.DataFrame(\n",
    "                self.scaler.transform(X_full),\n",
    "                columns=self.feature_names,\n",
    "                index=X_full.index\n",
    "            )\n",
    "            predictions = self.model.predict(X_full_scaled)\n",
    "        else:\n",
    "            predictions = self.model.predict(X_full)\n",
    "        \n",
    "        if include_all_data:\n",
    "            output_df = self.data.copy()\n",
    "        else:\n",
    "            output_df = pd.DataFrame()\n",
    "            output_df[self.target_col] = self.data[self.target_col]\n",
    "        \n",
    "        output_df['predicted'] = predictions\n",
    "        \n",
    "        output_df.to_csv(output_filepath, index=False)\n",
    "        print(f\"Predictions exported to: {output_filepath}\")\n",
    "        \n",
    "        return output_df\n",
    "    \n",
    "    def run_full_pipeline(self, input_filepath, output_filepath, include_all_data=True):\n",
    "        \"\"\"\n",
    "        Run the complete pipeline: load, preprocess, train, evaluate, and export.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        input_filepath : str\n",
    "            Path to input CSV file\n",
    "        output_filepath : str\n",
    "            Path for output CSV file with predictions\n",
    "        include_all_data : bool, default=True\n",
    "            Passed to export_predictions to control output file content.\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict : Performance metrics\n",
    "        \"\"\"\n",
    "        print(\"Starting full pipeline...\\n\")\n",
    "        \n",
    "        self.load_data(input_filepath)\n",
    "        self.preprocess_data()\n",
    "        self.train()\n",
    "        metrics = self.evaluate()\n",
    "        self.export_predictions(output_filepath, include_all_data=include_all_data)\n",
    "        \n",
    "        print(\"Pipeline completed successfully!\")\n",
    "        return metrics\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Configure the model with all parameters during initialization\n",
    "    clf = CardioNaiveBayesClassifier(\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        var_smoothing=1e-9,\n",
    "        scale_features=True,\n",
    "        target_col='cardio',  # <--- Parameter moved here\n",
    "        drop_cols=['id']      # <--- Parameter moved here\n",
    "    )\n",
    "    \n",
    "    # Run the pipeline with just the filepaths\n",
    "    metrics = clf.run_full_pipeline(\n",
    "        input_filepath='train.csv',\n",
    "        output_filepath='cardio_train_pred_nb.csv' # Changed output name\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6342e2",
   "metadata": {},
   "source": [
    "# Gradient Boosting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb750b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting full pipeline...\n",
      "\n",
      "Data loaded successfully: 68520 rows, 13 columns\n",
      "Data preprocessed: Train size = 54816, Test size = 13704\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                             f1_score, confusion_matrix, classification_report)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class CardioGradientBoostingClassifier:\n",
    "    \"\"\"\n",
    "    A reusable Gradient Boosting classifier for cardiovascular disease prediction.\n",
    "    \n",
    "    Features:\n",
    "    - Train/test split with customizable ratio\n",
    "    - Optional feature scaling\n",
    "    - Comprehensive performance metrics\n",
    "    - Export predictions to CSV\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, test_size=0.2, random_state=42, n_estimators=100, \n",
    "                 learning_rate=0.1, max_depth=3, scale_features=True,\n",
    "                 target_col='cardio', drop_cols=None):\n",
    "        \"\"\"\n",
    "        Initialize the classifier.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        test_size : float, default=0.2\n",
    "            Proportion of dataset to include in test split\n",
    "        random_state : int, default=42\n",
    "            Random state for reproducibility\n",
    "        n_estimators : int, default=100\n",
    "            The number of boosting stages to perform\n",
    "        learning_rate : float, default=0.1\n",
    "            Learning rate shrinks the contribution of each tree by learning_rate\n",
    "        max_depth : int, default=3\n",
    "            Maximum depth of the individual regression estimators\n",
    "        scale_features : bool, default=True\n",
    "            Whether to scale features\n",
    "        target_col : str, default='cardio'\n",
    "            Name of the target variable column\n",
    "        drop_cols : list, default=None\n",
    "            List of columns to drop (e.g., ID columns)\n",
    "        \"\"\"\n",
    "\n",
    "        self.test_size = test_size\n",
    "        self.random_state = random_state\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.scale_features = scale_features\n",
    "        self.target_col = target_col\n",
    "        self.drop_cols = drop_cols if drop_cols else []\n",
    "        \n",
    "        self.model = None\n",
    "        self.scaler = None\n",
    "        self.data = None\n",
    "        self.X_train = None\n",
    "        self.X_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "        self.feature_names = None\n",
    "        self.metrics = {}\n",
    "        \n",
    "    def load_data(self, filepath):\n",
    "        \"\"\"\n",
    "        Load data from CSV file.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        filepath : str\n",
    "            Path to the CSV file\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pd.DataFrame : Loaded dataframe\n",
    "        \"\"\"\n",
    "        self.data = pd.read_csv(filepath)\n",
    "        print(f\"Data loaded successfully: {self.data.shape[0]} rows, {self.data.shape[1]} columns\")\n",
    "        return self.data\n",
    "    \n",
    "    def preprocess_data(self):\n",
    "        \"\"\"\n",
    "        Preprocess the data for training.\n",
    "        Uses `self.target_col` and `self.drop_cols` from initialization.\n",
    "        \"\"\"\n",
    "        if self.data is None:\n",
    "            raise ValueError(\"No data loaded. Call load_data() first.\")\n",
    "        \n",
    "        df = self.data.copy()\n",
    "        if self.drop_cols:\n",
    "            df = df.drop(columns=self.drop_cols, errors='ignore')\n",
    "        \n",
    "        X = df.drop(columns=[self.target_col])\n",
    "        y = df[self.target_col]\n",
    "        \n",
    "        self.feature_names = X.columns.tolist()\n",
    "        \n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            X, y, test_size=self.test_size, random_state=self.random_state, stratify=y\n",
    "        )\n",
    "        \n",
    "        if self.scale_features:\n",
    "            self.scaler = StandardScaler()\n",
    "            self.X_train = pd.DataFrame(\n",
    "                self.scaler.fit_transform(self.X_train),\n",
    "                columns=self.feature_names,\n",
    "                index=self.X_train.index\n",
    "            )\n",
    "            self.X_test = pd.DataFrame(\n",
    "                self.scaler.transform(self.X_test),\n",
    "                columns=self.feature_names,\n",
    "                index=self.X_test.index\n",
    "            )\n",
    "        \n",
    "        print(f\"Data preprocessed: Train size = {len(self.X_train)}, Test size = {len(self.X_test)}\")\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Train the Gradient Boosting classifier.\"\"\"\n",
    "        if self.X_train is None:\n",
    "            raise ValueError(\"Data not preprocessed. Call preprocess_data() first.\")\n",
    "        \n",
    "        self.model = GradientBoostingClassifier(\n",
    "            n_estimators=self.n_estimators,\n",
    "            learning_rate=self.learning_rate,\n",
    "            max_depth=self.max_depth,\n",
    "            random_state=self.random_state\n",
    "        )\n",
    "        \n",
    "        self.model.fit(self.X_train, self.y_train)\n",
    "        print(\"Model trained successfully!\")\n",
    "    \n",
    "    def predict(self, X=None):\n",
    "        \"\"\"\n",
    "        Make predictions on data.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : pd.DataFrame, default=None\n",
    "            Data to predict. If None, uses test set.\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        np.ndarray : Predictions\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained. Call train() first.\")\n",
    "        \n",
    "        if X is None:\n",
    "            X_to_predict = self.X_test\n",
    "        else:\n",
    "            X_to_predict = X\n",
    "\n",
    "        if X is not None and self.scale_features and self.scaler is not None:\n",
    "            # When predicting on new, external data\n",
    "            X_to_predict = pd.DataFrame(\n",
    "                self.scaler.transform(X_to_predict),\n",
    "                columns=X_to_predict.columns,\n",
    "                index=X_to_predict.index\n",
    "            )\n",
    "        \n",
    "        return self.model.predict(X_to_predict)\n",
    "    \n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        Evaluate model performance on test set.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict : Dictionary containing all performance metrics\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained. Call train() first.\")\n",
    "        \n",
    "        y_pred = self.predict()\n",
    "        \n",
    "        self.metrics = {\n",
    "            'accuracy': accuracy_score(self.y_test, y_pred),\n",
    "            'precision': precision_score(self.y_test, y_pred, average='binary'),\n",
    "            'recall': recall_score(self.y_test, y_pred, average='binary'),\n",
    "            'f1_score': f1_score(self.y_test, y_pred, average='binary'),\n",
    "            'confusion_matrix': confusion_matrix(self.y_test, y_pred)\n",
    "        }\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"MODEL PERFORMANCE METRICS\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"Accuracy:  {self.metrics['accuracy']:.4f}\")\n",
    "        print(f\"Precision: {self.metrics['precision']:.4f}\")\n",
    "        print(f\"Recall:    {self.metrics['recall']:.4f}\")\n",
    "        print(f\"F1-Score:  {self.metrics['f1_score']:.4f}\")\n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        print(self.metrics['confusion_matrix'])\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(self.y_test, y_pred, \n",
    "                                      target_names=['No Disease', 'Disease']))\n",
    "        print(\"=\"*50 + \"\\n\")\n",
    "        \n",
    "        return self.metrics\n",
    "    \n",
    "    def export_predictions(self, output_filepath, include_all_data=True):\n",
    "        \"\"\"\n",
    "        Export predictions for the *entire* loaded dataset to a new CSV file.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        output_filepath : str\n",
    "            Path for the output CSV file\n",
    "        include_all_data : bool, default=True\n",
    "            If True, includes all original columns. If False, only includes\n",
    "            the target and predicted columns.\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained. Call train() first.\")\n",
    "        if self.data is None:\n",
    "            raise ValueError(\"No data loaded. Call load_data() first.\")\n",
    "            \n",
    "        \n",
    "        # Prepare the full dataset for prediction\n",
    "        X_full = self.data.copy()\n",
    "        \n",
    "        if self.drop_cols:\n",
    "            X_full = X_full.drop(columns=self.drop_cols, errors='ignore')\n",
    "            \n",
    "        X_full = X_full.drop(columns=[self.target_col], errors='ignore')\n",
    "        \n",
    "        # Ensure columns match training data\n",
    "        X_full = X_full[self.feature_names]\n",
    "        \n",
    "        if self.scale_features and self.scaler is not None:\n",
    "            X_full_scaled = pd.DataFrame(\n",
    "                self.scaler.transform(X_full),\n",
    "                columns=self.feature_names,\n",
    "                index=X_full.index\n",
    "            )\n",
    "            predictions = self.model.predict(X_full_scaled)\n",
    "        else:\n",
    "            predictions = self.model.predict(X_full)\n",
    "        \n",
    "        if include_all_data:\n",
    "            output_df = self.data.copy()\n",
    "        else:\n",
    "            output_df = pd.DataFrame()\n",
    "            output_df[self.target_col] = self.data[self.target_col]\n",
    "        \n",
    "        output_df['predicted'] = predictions\n",
    "        \n",
    "        output_df.to_csv(output_filepath, index=False)\n",
    "        print(f\"Predictions exported to: {output_filepath}\")\n",
    "        \n",
    "        return output_df\n",
    "    \n",
    "    def run_full_pipeline(self, input_filepath, output_filepath, include_all_data=True):\n",
    "        \"\"\"\n",
    "        Run the complete pipeline: load, preprocess, train, evaluate, and export.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        input_filepath : str\n",
    "            Path to input CSV file\n",
    "        output_filepath : str\n",
    "            Path for output CSV file with predictions\n",
    "        include_all_data : bool, default=True\n",
    "            Passed to export_predictions to control output file content.\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict : Performance metrics\n",
    "        \"\"\"\n",
    "        print(\"Starting full pipeline...\\n\")\n",
    "        \n",
    "        self.load_data(input_filepath)\n",
    "        self.preprocess_data()\n",
    "        self.train()\n",
    "        metrics = self.evaluate()\n",
    "        self.export_predictions(output_filepath, include_all_data=include_all_data)\n",
    "        \n",
    "        print(\"Pipeline completed successfully!\")\n",
    "        return metrics\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Configure the model with all parameters during initialization\n",
    "    clf = CardioGradientBoostingClassifier(\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        n_estimators=1000,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=100,\n",
    "        scale_features=True,\n",
    "        target_col='cardio',  # <--- Parameter moved here\n",
    "        drop_cols=['id']      # <--- Parameter moved here\n",
    "    )\n",
    "    \n",
    "    # Run the pipeline with just the filepaths\n",
    "    metrics = clf.run_full_pipeline(\n",
    "        input_filepath='train.csv',\n",
    "        output_filepath='cardio_train_pred_gb.csv' # Changed output name\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436d4908",
   "metadata": {},
   "source": [
    "# SVM \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77ba8a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting full pipeline...\n",
      "\n",
      "Data loaded successfully: 70000 rows, 13 columns\n",
      "Data preprocessed: Train size = 56000, Test size = 14000\n",
      "Model trained successfully!\n",
      "\n",
      "==================================================\n",
      "MODEL PERFORMANCE METRICS\n",
      "==================================================\n",
      "Accuracy:  0.7248\n",
      "Precision: 0.7394\n",
      "Recall:    0.6938\n",
      "F1-Score:  0.7159\n",
      "\n",
      "Confusion Matrix:\n",
      "[[5293 1711]\n",
      " [2142 4854]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  No Disease       0.71      0.76      0.73      7004\n",
      "     Disease       0.74      0.69      0.72      6996\n",
      "\n",
      "    accuracy                           0.72     14000\n",
      "   macro avg       0.73      0.72      0.72     14000\n",
      "weighted avg       0.73      0.72      0.72     14000\n",
      "\n",
      "==================================================\n",
      "\n",
      "Predictions exported to: cardio_train_pred.csv\n",
      "Pipeline completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                             f1_score, confusion_matrix, classification_report)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class CardioSupportVectorClassifier:\n",
    "    \"\"\"\n",
    "    A reusable Support Vector Machine classifier for cardiovascular disease prediction.\n",
    "    \n",
    "    Features:\n",
    "    - Train/test split with customizable ratio\n",
    "    - Optional feature scaling\n",
    "    - Comprehensive performance metrics\n",
    "    - Export predictions to CSV\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, test_size=0.2, random_state=42, C=1.0, \n",
    "                 kernel='rbf', gamma='scale', scale_features=True):\n",
    "        \"\"\"\n",
    "        Initialize the classifier.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        test_size : float, default=0.2\n",
    "            Proportion of dataset to include in test split\n",
    "        random_state : int, default=42\n",
    "            Random state for reproducibility\n",
    "        C : float, default=1.0\n",
    "            Regularization parameter. The strength of the regularization is inversely proportional to C\n",
    "        kernel : str, default='rbf'\n",
    "            Specifies the kernel type to be used in the algorithm\n",
    "        gamma : float or str, default='scale'\n",
    "            Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’\n",
    "        scale_features : bool, default=True\n",
    "            Whether to scale features\n",
    "        \"\"\"\n",
    "\n",
    "        self.test_size = test_size\n",
    "        self.random_state = random_state\n",
    "        self.C = C\n",
    "        self.kernel = kernel\n",
    "        self.gamma = gamma\n",
    "        self.scale_features = scale_features\n",
    "        \n",
    "        self.model = None\n",
    "        self.scaler = None\n",
    "        self.X_train = None\n",
    "        self.X_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "        self.feature_names = None\n",
    "        self.metrics = {}\n",
    "        self.drop_cols = None\n",
    "        \n",
    "    def load_data(self, filepath):\n",
    "        \"\"\"\n",
    "        Load data from CSV file.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        filepath : str\n",
    "            Path to the CSV file\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pd.DataFrame : Loaded dataframe\n",
    "        \"\"\"\n",
    "        self.data = pd.read_csv(filepath)\n",
    "        print(f\"Data loaded successfully: {self.data.shape[0]} rows, {self.data.shape[1]} columns\")\n",
    "        return self.data\n",
    "    \n",
    "    def preprocess_data(self, target_col='cardio', drop_cols=None):\n",
    "        \"\"\"\n",
    "        Preprocess the data for training.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        target_col : str, default='cardio'\n",
    "            Name of the target variable column\n",
    "        drop_cols : list, default=None\n",
    "            List of columns to drop (e.g., ID columns)\n",
    "        \"\"\"\n",
    "        if self.data is None:\n",
    "            raise ValueError(\"No data loaded. Call load_data() first.\")\n",
    "        \n",
    "        self.drop_cols = drop_cols if drop_cols else []\n",
    "        \n",
    "        df = self.data.copy()\n",
    "        if drop_cols:\n",
    "            df = df.drop(columns=drop_cols, errors='ignore')\n",
    "        \n",
    "        X = df.drop(columns=[target_col])\n",
    "        y = df[target_col]\n",
    "        \n",
    "        self.feature_names = X.columns.tolist()\n",
    "        \n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            X, y, test_size=self.test_size, random_state=self.random_state, stratify=y\n",
    "        )\n",
    "        \n",
    "        if self.scale_features:\n",
    "            self.scaler = StandardScaler()\n",
    "            self.X_train = pd.DataFrame(\n",
    "                self.scaler.fit_transform(self.X_train),\n",
    "                columns=self.feature_names,\n",
    "                index=self.X_train.index\n",
    "            )\n",
    "            self.X_test = pd.DataFrame(\n",
    "                self.scaler.transform(self.X_test),\n",
    "                columns=self.feature_names,\n",
    "                index=self.X_test.index\n",
    "            )\n",
    "        \n",
    "        print(f\"Data preprocessed: Train size = {len(self.X_train)}, Test size = {len(self.X_test)}\")\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Train the Support Vector Machine classifier.\"\"\"\n",
    "        if self.X_train is None:\n",
    "            raise ValueError(\"Data not preprocessed. Call preprocess_data() first.\")\n",
    "        \n",
    "        self.model = SVC(\n",
    "            C=self.C,\n",
    "            kernel=self.kernel,\n",
    "            gamma=self.gamma,\n",
    "            random_state=self.random_state\n",
    "        )\n",
    "        \n",
    "        self.model.fit(self.X_train, self.y_train)\n",
    "        print(\"Model trained successfully!\")\n",
    "    \n",
    "    def predict(self, X=None):\n",
    "        \"\"\"\n",
    "        Make predictions on data.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : pd.DataFrame, default=None\n",
    "            Data to predict. If None, uses test set.\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        np.ndarray : Predictions\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained. Call train() first.\")\n",
    "        \n",
    "        if X is None:\n",
    "            X = self.X_test\n",
    "        elif self.scale_features and self.scaler is not None:\n",
    "            X = pd.DataFrame(\n",
    "                self.scaler.transform(X),\n",
    "                columns=X.columns,\n",
    "                index=X.index\n",
    "            )\n",
    "        \n",
    "        return self.model.predict(X)\n",
    "    \n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        Evaluate model performance on test set.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict : Dictionary containing all performance metrics\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained. Call train() first.\")\n",
    "        \n",
    "        y_pred = self.predict()\n",
    "        \n",
    "        self.metrics = {\n",
    "            'accuracy': accuracy_score(self.y_test, y_pred),\n",
    "            'precision': precision_score(self.y_test, y_pred, average='binary'),\n",
    "            'recall': recall_score(self.y_test, y_pred, average='binary'),\n",
    "            'f1_score': f1_score(self.y_test, y_pred, average='binary'),\n",
    "            'confusion_matrix': confusion_matrix(self.y_test, y_pred)\n",
    "        }\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"MODEL PERFORMANCE METRICS\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"Accuracy:  {self.metrics['accuracy']:.4f}\")\n",
    "        print(f\"Precision: {self.metrics['precision']:.4f}\")\n",
    "        print(f\"Recall:    {self.metrics['recall']:.4f}\")\n",
    "        print(f\"F1-Score:  {self.metrics['f1_score']:.4f}\")\n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        print(self.metrics['confusion_matrix'])\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(self.y_test, y_pred, \n",
    "                                   target_names=['No Disease', 'Disease']))\n",
    "        print(\"=\"*50 + \"\\n\")\n",
    "        \n",
    "        return self.metrics\n",
    "    \n",
    "    def export_predictions(self, output_filepath, include_all_data=True):\n",
    "        \"\"\"\n",
    "        Export predictions to a new CSV file.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        output_filepath : str\n",
    "            Path for the output CSV file\n",
    "        include_all_data : bool, default=True\n",
    "            If True, includes all original columns. If False, only includes\n",
    "            the target and predicted columns.\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained. Call train() first.\")\n",
    "        \n",
    "        X_full = self.data.copy()\n",
    "        \n",
    "        if self.drop_cols:\n",
    "            X_full = X_full.drop(columns=self.drop_cols, errors='ignore')\n",
    "        \n",
    "        X_full = X_full.drop(columns=['cardio'], errors='ignore')\n",
    "        \n",
    "        if self.scale_features and self.scaler is not None:\n",
    "            X_full_scaled = pd.DataFrame(\n",
    "                self.scaler.transform(X_full),\n",
    "                columns=self.feature_names,\n",
    "                index=X_full.index\n",
    "            )\n",
    "            predictions = self.model.predict(X_full_scaled)\n",
    "        else:\n",
    "            predictions = self.model.predict(X_full)\n",
    "        \n",
    "        if include_all_data:\n",
    "            output_df = self.data.copy()\n",
    "        else:\n",
    "            output_df = pd.DataFrame()\n",
    "            output_df['cardio'] = self.data['cardio']\n",
    "        \n",
    "        output_df['predicted'] = predictions\n",
    "        \n",
    "        output_df.to_csv(output_filepath, index=False)\n",
    "        print(f\"Predictions exported to: {output_filepath}\")\n",
    "        \n",
    "        return output_df\n",
    "    \n",
    "    def run_full_pipeline(self, input_filepath, output_filepath, \n",
    "                         target_col='cardio', drop_cols=None):\n",
    "        \"\"\"\n",
    "        Run the complete pipeline: load, preprocess, train, evaluate, and export.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        input_filepath : str\n",
    "            Path to input CSV file\n",
    "        output_filepath : str\n",
    "            Path for output CSV file with predictions\n",
    "        target_col : str, default='cardio'\n",
    "            Name of target variable\n",
    "        drop_cols : list, default=None\n",
    "            Columns to drop before training\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict : Performance metrics\n",
    "        \"\"\"\n",
    "        print(\"Starting full pipeline...\\n\")\n",
    "        \n",
    "        self.load_data(input_filepath)\n",
    "        self.preprocess_data(target_col=target_col, drop_cols=drop_cols)\n",
    "        self.train()\n",
    "        metrics = self.evaluate()\n",
    "        self.export_predictions(output_filepath)\n",
    "        \n",
    "        print(\"Pipeline completed successfully!\")\n",
    "        return metrics\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    clf = CardioSupportVectorClassifier(\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        C=1.0,\n",
    "        kernel='rbf',\n",
    "        gamma='scale',\n",
    "        scale_features=True\n",
    "    )\n",
    "    \n",
    "    metrics = clf.run_full_pipeline(\n",
    "        input_filepath='cardio_train.csv',\n",
    "        output_filepath='cardio_train_pred.csv',\n",
    "        drop_cols=['id'] \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c68c01",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33c9fd66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting full pipeline...\n",
      "\n",
      "Data loaded successfully: 70000 rows, 13 columns\n",
      "Data preprocessed: Train size = 56000, Test size = 14000\n",
      "Model trained successfully!\n",
      "\n",
      "==================================================\n",
      "MODEL PERFORMANCE METRICS\n",
      "==================================================\n",
      "Accuracy:  0.6325\n",
      "Precision: 0.6363\n",
      "Recall:    0.6176\n",
      "F1-Score:  0.6268\n",
      "\n",
      "Confusion Matrix:\n",
      "[[4534 2470]\n",
      " [2675 4321]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  No Disease       0.63      0.65      0.64      7004\n",
      "     Disease       0.64      0.62      0.63      6996\n",
      "\n",
      "    accuracy                           0.63     14000\n",
      "   macro avg       0.63      0.63      0.63     14000\n",
      "weighted avg       0.63      0.63      0.63     14000\n",
      "\n",
      "==================================================\n",
      "\n",
      "Predictions exported to: cardio_train_predknn.csv\n",
      "Pipeline completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                             f1_score, confusion_matrix, classification_report)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class CardioKNeighborsClassifier:\n",
    "    \"\"\"\n",
    "    A reusable K-Nearest Neighbors classifier for cardiovascular disease prediction.\n",
    "    \n",
    "    Features:\n",
    "    - Train/test split with customizable ratio\n",
    "    - Optional feature scaling\n",
    "    - Comprehensive performance metrics\n",
    "    - Export predictions to CSV\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, test_size=0.2, random_state=42, n_neighbors=5, \n",
    "                 weights='uniform', metric='euclidean', scale_features=True):\n",
    "        \"\"\"\n",
    "        Initialize the classifier.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        test_size : float, default=0.2\n",
    "            Proportion of dataset to include in test split\n",
    "        random_state : int, default=42\n",
    "            Random state for reproducibility\n",
    "        n_neighbors : int, default=5\n",
    "            Number of neighbors to use by default for kneighbors queries\n",
    "        weights : str, default='uniform'\n",
    "            Weight function used in prediction\n",
    "        metric : str, default='euclidean'\n",
    "            Distance metric to use for the tree\n",
    "        scale_features : bool, default=True\n",
    "            Whether to scale features\n",
    "        \"\"\"\n",
    "\n",
    "        self.test_size = test_size\n",
    "        self.random_state = random_state\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.weights = weights\n",
    "        self.metric = metric\n",
    "        self.scale_features = scale_features\n",
    "        \n",
    "        self.model = None\n",
    "        self.scaler = None\n",
    "        self.X_train = None\n",
    "        self.X_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "        self.feature_names = None\n",
    "        self.metrics = {}\n",
    "        self.drop_cols = None\n",
    "        \n",
    "    def load_data(self, filepath):\n",
    "        \"\"\"\n",
    "        Load data from CSV file.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        filepath : str\n",
    "            Path to the CSV file\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pd.DataFrame : Loaded dataframe\n",
    "        \"\"\"\n",
    "        self.data = pd.read_csv(filepath)\n",
    "        print(f\"Data loaded successfully: {self.data.shape[0]} rows, {self.data.shape[1]} columns\")\n",
    "        return self.data\n",
    "    \n",
    "    def preprocess_data(self, target_col='cardio', drop_cols=None):\n",
    "        \"\"\"\n",
    "        Preprocess the data for training.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        target_col : str, default='cardio'\n",
    "            Name of the target variable column\n",
    "        drop_cols : list, default=None\n",
    "            List of columns to drop (e.g., ID columns)\n",
    "        \"\"\"\n",
    "        if self.data is None:\n",
    "            raise ValueError(\"No data loaded. Call load_data() first.\")\n",
    "        \n",
    "        self.drop_cols = drop_cols if drop_cols else []\n",
    "        \n",
    "        df = self.data.copy()\n",
    "        if drop_cols:\n",
    "            df = df.drop(columns=drop_cols, errors='ignore')\n",
    "        \n",
    "        X = df.drop(columns=[target_col])\n",
    "        y = df[target_col]\n",
    "        \n",
    "        self.feature_names = X.columns.tolist()\n",
    "        \n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            X, y, test_size=self.test_size, random_state=self.random_state, stratify=y\n",
    "        )\n",
    "        \n",
    "        if self.scale_features:\n",
    "            self.scaler = StandardScaler()\n",
    "            self.X_train = pd.DataFrame(\n",
    "                self.scaler.fit_transform(self.X_train),\n",
    "                columns=self.feature_names,\n",
    "                index=self.X_train.index\n",
    "            )\n",
    "            self.X_test = pd.DataFrame(\n",
    "                self.scaler.transform(self.X_test),\n",
    "                columns=self.feature_names,\n",
    "                index=self.X_test.index\n",
    "            )\n",
    "        \n",
    "        print(f\"Data preprocessed: Train size = {len(self.X_train)}, Test size = {len(self.X_test)}\")\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Train the K-Nearest Neighbors classifier.\"\"\"\n",
    "        if self.X_train is None:\n",
    "            raise ValueError(\"Data not preprocessed. Call preprocess_data() first.\")\n",
    "        \n",
    "        self.model = KNeighborsClassifier(\n",
    "            n_neighbors=self.n_neighbors,\n",
    "            weights=self.weights,\n",
    "            metric=self.metric\n",
    "        )\n",
    "        \n",
    "        self.model.fit(self.X_train, self.y_train)\n",
    "        print(\"Model trained successfully!\")\n",
    "    \n",
    "    def predict(self, X=None):\n",
    "        \"\"\"\n",
    "        Make predictions on data.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : pd.DataFrame, default=None\n",
    "            Data to predict. If None, uses test set.\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        np.ndarray : Predictions\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained. Call train() first.\")\n",
    "        \n",
    "        if X is None:\n",
    "            X = self.X_test\n",
    "        elif self.scale_features and self.scaler is not None:\n",
    "            X = pd.DataFrame(\n",
    "                self.scaler.transform(X),\n",
    "                columns=X.columns,\n",
    "                index=X.index\n",
    "            )\n",
    "        \n",
    "        return self.model.predict(X)\n",
    "    \n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        Evaluate model performance on test set.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict : Dictionary containing all performance metrics\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained. Call train() first.\")\n",
    "        \n",
    "        y_pred = self.predict()\n",
    "        \n",
    "        self.metrics = {\n",
    "            'accuracy': accuracy_score(self.y_test, y_pred),\n",
    "            'precision': precision_score(self.y_test, y_pred, average='binary'),\n",
    "            'recall': recall_score(self.y_test, y_pred, average='binary'),\n",
    "            'f1_score': f1_score(self.y_test, y_pred, average='binary'),\n",
    "            'confusion_matrix': confusion_matrix(self.y_test, y_pred)\n",
    "        }\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"MODEL PERFORMANCE METRICS\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"Accuracy:  {self.metrics['accuracy']:.4f}\")\n",
    "        print(f\"Precision: {self.metrics['precision']:.4f}\")\n",
    "        print(f\"Recall:    {self.metrics['recall']:.4f}\")\n",
    "        print(f\"F1-Score:  {self.metrics['f1_score']:.4f}\")\n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        print(self.metrics['confusion_matrix'])\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(self.y_test, y_pred, \n",
    "                                   target_names=['No Disease', 'Disease']))\n",
    "        print(\"=\"*50 + \"\\n\")\n",
    "        \n",
    "        return self.metrics\n",
    "    \n",
    "    def export_predictions(self, output_filepath, include_all_data=True):\n",
    "        \"\"\"\n",
    "        Export predictions to a new CSV file.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        output_filepath : str\n",
    "            Path for the output CSV file\n",
    "        include_all_data : bool, default=True\n",
    "            If True, includes all original columns. If False, only includes\n",
    "            the target and predicted columns.\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained. Call train() first.\")\n",
    "        \n",
    "        X_full = self.data.copy()\n",
    "        \n",
    "        if self.drop_cols:\n",
    "            X_full = X_full.drop(columns=self.drop_cols, errors='ignore')\n",
    "        \n",
    "        X_full = X_full.drop(columns=['cardio'], errors='ignore')\n",
    "        \n",
    "        if self.scale_features and self.scaler is not None:\n",
    "            X_full_scaled = pd.DataFrame(\n",
    "                self.scaler.transform(X_full),\n",
    "                columns=self.feature_names,\n",
    "                index=X_full.index\n",
    "            )\n",
    "            predictions = self.model.predict(X_full_scaled)\n",
    "        else:\n",
    "            predictions = self.model.predict(X_full)\n",
    "        \n",
    "        if include_all_data:\n",
    "            output_df = self.data.copy()\n",
    "        else:\n",
    "            output_df = pd.DataFrame()\n",
    "            output_df['cardio'] = self.data['cardio']\n",
    "        \n",
    "        output_df['predicted'] = predictions\n",
    "        \n",
    "        output_df.to_csv(output_filepath, index=False)\n",
    "        print(f\"Predictions exported to: {output_filepath}\")\n",
    "        \n",
    "        return output_df\n",
    "    \n",
    "    def run_full_pipeline(self, input_filepath, output_filepath, \n",
    "                         target_col='cardio', drop_cols=None):\n",
    "        \"\"\"\n",
    "        Run the complete pipeline: load, preprocess, train, evaluate, and export.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        input_filepath : str\n",
    "            Path to input CSV file\n",
    "        output_filepath : str\n",
    "            Path for output CSV file with predictions\n",
    "        target_col : str, default='cardio'\n",
    "            Name of target variable\n",
    "        drop_cols : list, default=None\n",
    "            Columns to drop before training\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict : Performance metrics\n",
    "        \"\"\"\n",
    "        print(\"Starting full pipeline...\\n\")\n",
    "        \n",
    "        self.load_data(input_filepath)\n",
    "        self.preprocess_data(target_col=target_col, drop_cols=drop_cols)\n",
    "        self.train()\n",
    "        metrics = self.evaluate()\n",
    "        self.export_predictions(output_filepath)\n",
    "        \n",
    "        print(\"Pipeline completed successfully!\")\n",
    "        return metrics\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    clf = CardioKNeighborsClassifier(\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        n_neighbors=3,\n",
    "        weights='uniform',\n",
    "        metric='euclidean',\n",
    "        scale_features=True\n",
    "    )\n",
    "    \n",
    "    metrics = clf.run_full_pipeline(\n",
    "        input_filepath='cardio_train.csv',\n",
    "        output_filepath='cardio_train_predknn.csv',\n",
    "        drop_cols=['id'] \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b771e70",
   "metadata": {},
   "source": [
    "# DL Algorithms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d9473896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement torch (from versions: none)\n",
      "ERROR: No matching distribution found for torch\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moptim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01moptim\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                             f1_score, confusion_matrix, classification_report)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class CardioMLPClassifier:\n",
    "    \"\"\"\n",
    "    A reusable Multi-Layer Perceptron (MLP) deep learning classifier for cardiovascular disease prediction using PyTorch.\n",
    "    \n",
    "    Features:\n",
    "    - Train/test split with customizable ratio\n",
    "    - Optional feature scaling\n",
    "    - Comprehensive performance metrics\n",
    "    - Export predictions to CSV\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, test_size=0.2, random_state=42, hidden_sizes=[64, 32], \n",
    "                 lr=0.001, epochs=100, batch_size=32, scale_features=True):\n",
    "        \"\"\"\n",
    "        Initialize the classifier.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        test_size : float, default=0.2\n",
    "            Proportion of dataset to include in test split\n",
    "        random_state : int, default=42\n",
    "            Random state for reproducibility\n",
    "        hidden_sizes : list, default=[64, 32]\n",
    "            List of hidden layer sizes\n",
    "        lr : float, default=0.001\n",
    "            Learning rate for the optimizer\n",
    "        epochs : int, default=100\n",
    "            Number of training epochs\n",
    "        batch_size : int, default=32\n",
    "            Batch size for training\n",
    "        scale_features : bool, default=True\n",
    "            Whether to scale features\n",
    "        \"\"\"\n",
    "\n",
    "        self.test_size = test_size\n",
    "        self.random_state = random_state\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.scale_features = scale_features\n",
    "        \n",
    "        self.model = None\n",
    "        self.scaler = None\n",
    "        self.X_train = None\n",
    "        self.X_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "        self.X_train_tensor = None\n",
    "        self.y_train_tensor = None\n",
    "        self.feature_names = None\n",
    "        self.metrics = {}\n",
    "        self.drop_cols = None\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "    def load_data(self, filepath):\n",
    "        \"\"\"\n",
    "        Load data from CSV file.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        filepath : str\n",
    "            Path to the CSV file\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pd.DataFrame : Loaded dataframe\n",
    "        \"\"\"\n",
    "        self.data = pd.read_csv(filepath)\n",
    "        print(f\"Data loaded successfully: {self.data.shape[0]} rows, {self.data.shape[1]} columns\")\n",
    "        return self.data\n",
    "    \n",
    "    def preprocess_data(self, target_col='cardio', drop_cols=None):\n",
    "        \"\"\"\n",
    "        Preprocess the data for training.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        target_col : str, default='cardio'\n",
    "            Name of the target variable column\n",
    "        drop_cols : list, default=None\n",
    "            List of columns to drop (e.g., ID columns)\n",
    "        \"\"\"\n",
    "        if self.data is None:\n",
    "            raise ValueError(\"No data loaded. Call load_data() first.\")\n",
    "        \n",
    "        self.drop_cols = drop_cols if drop_cols else []\n",
    "        \n",
    "        df = self.data.copy()\n",
    "        if drop_cols:\n",
    "            df = df.drop(columns=drop_cols, errors='ignore')\n",
    "        \n",
    "        X = df.drop(columns=[target_col])\n",
    "        y = df[target_col]\n",
    "        \n",
    "        self.feature_names = X.columns.tolist()\n",
    "        \n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            X, y, test_size=self.test_size, random_state=self.random_state, stratify=y\n",
    "        )\n",
    "        \n",
    "        if self.scale_features:\n",
    "            self.scaler = StandardScaler()\n",
    "            self.X_train = pd.DataFrame(\n",
    "                self.scaler.fit_transform(self.X_train),\n",
    "                columns=self.feature_names,\n",
    "                index=self.X_train.index\n",
    "            )\n",
    "            self.X_test = pd.DataFrame(\n",
    "                self.scaler.transform(self.X_test),\n",
    "                columns=self.feature_names,\n",
    "                index=self.X_test.index\n",
    "            )\n",
    "        \n",
    "        print(f\"Data preprocessed: Train size = {len(self.X_train)}, Test size = {len(self.X_test)}\")\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Train the MLP classifier.\"\"\"\n",
    "        if self.X_train is None:\n",
    "            raise ValueError(\"Data not preprocessed. Call preprocess_data() first.\")\n",
    "        \n",
    "        # Convert to tensors\n",
    "        X_train_tensor = torch.FloatTensor(self.X_train.values).to(self.device)\n",
    "        y_train_tensor = torch.LongTensor(self.y_train.values).to(self.device)\n",
    "        \n",
    "        self.X_train_tensor = X_train_tensor\n",
    "        self.y_train_tensor = y_train_tensor\n",
    "        \n",
    "        # Define model\n",
    "        input_size = len(self.feature_names)\n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        for size in self.hidden_sizes:\n",
    "            layers.append(nn.Linear(prev_size, size))\n",
    "            layers.append(nn.ReLU())\n",
    "            prev_size = size\n",
    "        layers.append(nn.Linear(prev_size, 2))  # Binary classification\n",
    "        \n",
    "        self.model = nn.Sequential(*layers).to(self.device)\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        \n",
    "        # DataLoader\n",
    "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        \n",
    "        self.model.train()\n",
    "        for epoch in range(self.epochs):\n",
    "            running_loss = 0.0\n",
    "            for batch_x, batch_y in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "            \n",
    "            if (epoch + 1) % 50 == 0:\n",
    "                avg_loss = running_loss / len(train_loader)\n",
    "                print(f'Epoch [{epoch+1}/{self.epochs}], Loss: {avg_loss:.4f}')\n",
    "        \n",
    "        print(\"Model trained successfully!\")\n",
    "    \n",
    "    def predict(self, X=None):\n",
    "        \"\"\"\n",
    "        Make predictions on data.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : pd.DataFrame, default=None\n",
    "            Data to predict. If None, uses test set.\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        np.ndarray : Predictions\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained. Call train() first.\")\n",
    "        \n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            if X is None:\n",
    "                input_tensor = torch.FloatTensor(self.X_test.values).to(self.device)\n",
    "            else:\n",
    "                if self.scale_features and self.scaler is not None:\n",
    "                    X_scaled = pd.DataFrame(\n",
    "                        self.scaler.transform(X),\n",
    "                        columns=X.columns,\n",
    "                        index=X.index\n",
    "                    )\n",
    "                    input_tensor = torch.FloatTensor(X_scaled.values).to(self.device)\n",
    "                else:\n",
    "                    input_tensor = torch.FloatTensor(X.values).to(self.device)\n",
    "            \n",
    "            outputs = self.model(input_tensor)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            return predicted.cpu().numpy()\n",
    "    \n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        Evaluate model performance on test set.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict : Dictionary containing all performance metrics\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained. Call train() first.\")\n",
    "        \n",
    "        y_pred = self.predict()\n",
    "        \n",
    "        self.metrics = {\n",
    "            'accuracy': accuracy_score(self.y_test, y_pred),\n",
    "            'precision': precision_score(self.y_test, y_pred, average='binary'),\n",
    "            'recall': recall_score(self.y_test, y_pred, average='binary'),\n",
    "            'f1_score': f1_score(self.y_test, y_pred, average='binary'),\n",
    "            'confusion_matrix': confusion_matrix(self.y_test, y_pred)\n",
    "        }\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"MODEL PERFORMANCE METRICS\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"Accuracy:  {self.metrics['accuracy']:.4f}\")\n",
    "        print(f\"Precision: {self.metrics['precision']:.4f}\")\n",
    "        print(f\"Recall:    {self.metrics['recall']:.4f}\")\n",
    "        print(f\"F1-Score:  {self.metrics['f1_score']:.4f}\")\n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        print(self.metrics['confusion_matrix'])\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(self.y_test, y_pred, \n",
    "                                   target_names=['No Disease', 'Disease']))\n",
    "        print(\"=\"*50 + \"\\n\")\n",
    "        \n",
    "        return self.metrics\n",
    "    \n",
    "    def export_predictions(self, output_filepath, include_all_data=True):\n",
    "        \"\"\"\n",
    "        Export predictions to a new CSV file.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        output_filepath : str\n",
    "            Path for the output CSV file\n",
    "        include_all_data : bool, default=True\n",
    "            If True, includes all original columns. If False, only includes\n",
    "            the target and predicted columns.\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained. Call train() first.\")\n",
    "        \n",
    "        X_full = self.data.copy()\n",
    "        \n",
    "        if self.drop_cols:\n",
    "            X_full = X_full.drop(columns=self.drop_cols, errors='ignore')\n",
    "        \n",
    "        X_full = X_full.drop(columns=['cardio'], errors='ignore')\n",
    "        \n",
    "        if self.scale_features and self.scaler is not None:\n",
    "            X_full_scaled = pd.DataFrame(\n",
    "                self.scaler.transform(X_full),\n",
    "                columns=self.feature_names,\n",
    "                index=X_full.index\n",
    "            )\n",
    "            predictions = self.predict(X_full_scaled)\n",
    "        else:\n",
    "            predictions = self.predict(X_full)\n",
    "        \n",
    "        if include_all_data:\n",
    "            output_df = self.data.copy()\n",
    "        else:\n",
    "            output_df = pd.DataFrame()\n",
    "            output_df['cardio'] = self.data['cardio']\n",
    "        \n",
    "        output_df['predicted'] = predictions\n",
    "        \n",
    "        output_df.to_csv(output_filepath, index=False)\n",
    "        print(f\"Predictions exported to: {output_filepath}\")\n",
    "        \n",
    "        return output_df\n",
    "    \n",
    "    def run_full_pipeline(self, input_filepath, output_filepath, \n",
    "                         target_col='cardio', drop_cols=None):\n",
    "        \"\"\"\n",
    "        Run the complete pipeline: load, preprocess, train, evaluate, and export.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        input_filepath : str\n",
    "            Path to input CSV file\n",
    "        output_filepath : str\n",
    "            Path for output CSV file with predictions\n",
    "        target_col : str, default='cardio'\n",
    "            Name of target variable\n",
    "        drop_cols : list, default=None\n",
    "            Columns to drop before training\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict : Performance metrics\n",
    "        \"\"\"\n",
    "        print(\"Starting full pipeline...\\n\")\n",
    "        \n",
    "        self.load_data(input_filepath)\n",
    "        self.preprocess_data(target_col=target_col, drop_cols=drop_cols)\n",
    "        self.train()\n",
    "        metrics = self.evaluate()\n",
    "        self.export_predictions(output_filepath)\n",
    "        \n",
    "        print(\"Pipeline completed successfully!\")\n",
    "        return metrics\n",
    "\n",
    "\n",
    "class CardioTabularCNNClassifier:\n",
    "    \"\"\"\n",
    "    A reusable 1D Convolutional Neural Network (CNN) classifier for tabular cardiovascular disease prediction using PyTorch.\n",
    "    Treats features as a 1D sequence.\n",
    "    \n",
    "    Features:\n",
    "    - Train/test split with customizable ratio\n",
    "    - Optional feature scaling\n",
    "    - Comprehensive performance metrics\n",
    "    - Export predictions to CSV\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, test_size=0.2, random_state=42, filters=[32, 64], \n",
    "                 kernel_sizes=[3, 3], lr=0.001, epochs=100, batch_size=32, \n",
    "                 scale_features=True):\n",
    "        \"\"\"\n",
    "        Initialize the classifier.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        test_size : float, default=0.2\n",
    "            Proportion of dataset to include in test split\n",
    "        random_state : int, default=42\n",
    "            Random state for reproducibility\n",
    "        filters : list, default=[32, 64]\n",
    "            List of number of filters for each conv layer\n",
    "        kernel_sizes : list, default=[3, 3]\n",
    "            List of kernel sizes for each conv layer\n",
    "        lr : float, default=0.001\n",
    "            Learning rate for the optimizer\n",
    "        epochs : int, default=100\n",
    "            Number of training epochs\n",
    "        batch_size : int, default=32\n",
    "            Batch size for training\n",
    "        scale_features : bool, default=True\n",
    "            Whether to scale features\n",
    "        \"\"\"\n",
    "\n",
    "        self.test_size = test_size\n",
    "        self.random_state = random_state\n",
    "        self.filters = filters\n",
    "        self.kernel_sizes = kernel_sizes\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.scale_features = scale_features\n",
    "        \n",
    "        self.model = None\n",
    "        self.scaler = None\n",
    "        self.X_train = None\n",
    "        self.X_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "        self.X_train_tensor = None\n",
    "        self.y_train_tensor = None\n",
    "        self.feature_names = None\n",
    "        self.metrics = {}\n",
    "        self.drop_cols = None\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "    def load_data(self, filepath):\n",
    "        \"\"\"\n",
    "        Load data from CSV file.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        filepath : str\n",
    "            Path to the CSV file\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pd.DataFrame : Loaded dataframe\n",
    "        \"\"\"\n",
    "        self.data = pd.read_csv(filepath)\n",
    "        print(f\"Data loaded successfully: {self.data.shape[0]} rows, {self.data.shape[1]} columns\")\n",
    "        return self.data\n",
    "    \n",
    "    def preprocess_data(self, target_col='cardio', drop_cols=None):\n",
    "        \"\"\"\n",
    "        Preprocess the data for training.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        target_col : str, default='cardio'\n",
    "            Name of the target variable column\n",
    "        drop_cols : list, default=None\n",
    "            List of columns to drop (e.g., ID columns)\n",
    "        \"\"\"\n",
    "        if self.data is None:\n",
    "            raise ValueError(\"No data loaded. Call load_data() first.\")\n",
    "        \n",
    "        self.drop_cols = drop_cols if drop_cols else []\n",
    "        \n",
    "        df = self.data.copy()\n",
    "        if drop_cols:\n",
    "            df = df.drop(columns=drop_cols, errors='ignore')\n",
    "        \n",
    "        X = df.drop(columns=[target_col])\n",
    "        y = df[target_col]\n",
    "        \n",
    "        self.feature_names = X.columns.tolist()\n",
    "        \n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            X, y, test_size=self.test_size, random_state=self.random_state, stratify=y\n",
    "        )\n",
    "        \n",
    "        if self.scale_features:\n",
    "            self.scaler = StandardScaler()\n",
    "            self.X_train = pd.DataFrame(\n",
    "                self.scaler.fit_transform(self.X_train),\n",
    "                columns=self.feature_names,\n",
    "                index=self.X_train.index\n",
    "            )\n",
    "            self.X_test = pd.DataFrame(\n",
    "                self.scaler.transform(self.X_test),\n",
    "                columns=self.feature_names,\n",
    "                index=self.X_test.index\n",
    "            )\n",
    "        \n",
    "        print(f\"Data preprocessed: Train size = {len(self.X_train)}, Test size = {len(self.X_test)}\")\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Train the Tabular CNN classifier.\"\"\"\n",
    "        if self.X_train is None:\n",
    "            raise ValueError(\"Data not preprocessed. Call preprocess_data() first.\")\n",
    "        \n",
    "        # Convert to tensors with channel dimension for Conv1D\n",
    "        X_train_tensor = torch.FloatTensor(self.X_train.values).unsqueeze(1).to(self.device)  # (N, 1, features)\n",
    "        y_train_tensor = torch.LongTensor(self.y_train.values).to(self.device)\n",
    "        \n",
    "        self.X_train_tensor = X_train_tensor\n",
    "        self.y_train_tensor = y_train_tensor\n",
    "        \n",
    "        # Define model\n",
    "        input_size = len(self.feature_names)\n",
    "        conv_layers = []\n",
    "        prev_channels = 1\n",
    "        prev_size = input_size\n",
    "        for f, k in zip(self.filters, self.kernel_sizes):\n",
    "            conv_layers.append(nn.Conv1d(prev_channels, f, kernel_size=k, padding=(k-1)//2))\n",
    "            conv_layers.append(nn.ReLU())\n",
    "            conv_layers.append(nn.MaxPool1d(2))\n",
    "            prev_channels = f\n",
    "            prev_size = prev_size // 2  # Approximate after pooling\n",
    "        \n",
    "        # Flatten size calculation (approximate)\n",
    "        flatten_size = prev_channels * (input_size // 4)  # Assuming two poolings halve twice\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            *conv_layers,\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(flatten_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 2)\n",
    "        ).to(self.device)\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        \n",
    "        # DataLoader\n",
    "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        \n",
    "        self.model.train()\n",
    "        for epoch in range(self.epochs):\n",
    "            running_loss = 0.0\n",
    "            for batch_x, batch_y in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "            \n",
    "            if (epoch + 1) % 50 == 0:\n",
    "                avg_loss = running_loss / len(train_loader)\n",
    "                print(f'Epoch [{epoch+1}/{self.epochs}], Loss: {avg_loss:.4f}')\n",
    "        \n",
    "        print(\"Model trained successfully!\")\n",
    "    \n",
    "    def predict(self, X=None):\n",
    "        \"\"\"\n",
    "        Make predictions on data.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : pd.DataFrame, default=None\n",
    "            Data to predict. If None, uses test set.\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        np.ndarray : Predictions\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained. Call train() first.\")\n",
    "        \n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            if X is None:\n",
    "                input_tensor = torch.FloatTensor(self.X_test.values).unsqueeze(1).to(self.device)\n",
    "            else:\n",
    "                if self.scale_features and self.scaler is not None:\n",
    "                    X_scaled = pd.DataFrame(\n",
    "                        self.scaler.transform(X),\n",
    "                        columns=X.columns,\n",
    "                        index=X.index\n",
    "                    )\n",
    "                    input_tensor = torch.FloatTensor(X_scaled.values).unsqueeze(1).to(self.device)\n",
    "                else:\n",
    "                    input_tensor = torch.FloatTensor(X.values).unsqueeze(1).to(self.device)\n",
    "            \n",
    "            outputs = self.model(input_tensor)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            return predicted.cpu().numpy()\n",
    "    \n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        Evaluate model performance on test set.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict : Dictionary containing all performance metrics\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained. Call train() first.\")\n",
    "        \n",
    "        y_pred = self.predict()\n",
    "        \n",
    "        self.metrics = {\n",
    "            'accuracy': accuracy_score(self.y_test, y_pred),\n",
    "            'precision': precision_score(self.y_test, y_pred, average='binary'),\n",
    "            'recall': recall_score(self.y_test, y_pred, average='binary'),\n",
    "            'f1_score': f1_score(self.y_test, y_pred, average='binary'),\n",
    "            'confusion_matrix': confusion_matrix(self.y_test, y_pred)\n",
    "        }\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"MODEL PERFORMANCE METRICS\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"Accuracy:  {self.metrics['accuracy']:.4f}\")\n",
    "        print(f\"Precision: {self.metrics['precision']:.4f}\")\n",
    "        print(f\"Recall:    {self.metrics['recall']:.4f}\")\n",
    "        print(f\"F1-Score:  {self.metrics['f1_score']:.4f}\")\n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        print(self.metrics['confusion_matrix'])\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(self.y_test, y_pred, \n",
    "                                   target_names=['No Disease', 'Disease']))\n",
    "        print(\"=\"*50 + \"\\n\")\n",
    "        \n",
    "        return self.metrics\n",
    "    \n",
    "    def export_predictions(self, output_filepath, include_all_data=True):\n",
    "        \"\"\"\n",
    "        Export predictions to a new CSV file.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        output_filepath : str\n",
    "            Path for the output CSV file\n",
    "        include_all_data : bool, default=True\n",
    "            If True, includes all original columns. If False, only includes\n",
    "            the target and predicted columns.\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained. Call train() first.\")\n",
    "        \n",
    "        X_full = self.data.copy()\n",
    "        \n",
    "        if self.drop_cols:\n",
    "            X_full = X_full.drop(columns=self.drop_cols, errors='ignore')\n",
    "        \n",
    "        X_full = X_full.drop(columns=['cardio'], errors='ignore')\n",
    "        \n",
    "        if self.scale_features and self.scaler is not None:\n",
    "            X_full_scaled = pd.DataFrame(\n",
    "                self.scaler.transform(X_full),\n",
    "                columns=self.feature_names,\n",
    "                index=X_full.index\n",
    "            )\n",
    "            predictions = self.predict(X_full_scaled)\n",
    "        else:\n",
    "            predictions = self.predict(X_full)\n",
    "        \n",
    "        if include_all_data:\n",
    "            output_df = self.data.copy()\n",
    "        else:\n",
    "            output_df = pd.DataFrame()\n",
    "            output_df['cardio'] = self.data['cardio']\n",
    "        \n",
    "        output_df['predicted'] = predictions\n",
    "        \n",
    "        output_df.to_csv(output_filepath, index=False)\n",
    "        print(f\"Predictions exported to: {output_filepath}\")\n",
    "        \n",
    "        return output_df\n",
    "    \n",
    "    def run_full_pipeline(self, input_filepath, output_filepath, \n",
    "                         target_col='cardio', drop_cols=None):\n",
    "        \"\"\"\n",
    "        Run the complete pipeline: load, preprocess, train, evaluate, and export.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        input_filepath : str\n",
    "            Path to input CSV file\n",
    "        output_filepath : str\n",
    "            Path for output CSV file with predictions\n",
    "        target_col : str, default='cardio'\n",
    "            Name of target variable\n",
    "        drop_cols : list, default=None\n",
    "            Columns to drop before training\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict : Performance metrics\n",
    "        \"\"\"\n",
    "        print(\"Starting full pipeline...\\n\")\n",
    "        \n",
    "        self.load_data(input_filepath)\n",
    "        self.preprocess_data(target_col=target_col, drop_cols=drop_cols)\n",
    "        self.train()\n",
    "        metrics = self.evaluate()\n",
    "        self.export_predictions(output_filepath)\n",
    "        \n",
    "        print(\"Pipeline completed successfully!\")\n",
    "        return metrics\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # MLP Example\n",
    "    print(\"Running MLP Pipeline...\")\n",
    "    mlp_clf = CardioMLPClassifier(\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        hidden_sizes=[64, 32],\n",
    "        lr=0.001,\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        scale_features=True\n",
    "    )\n",
    "    \n",
    "    mlp_metrics = mlp_clf.run_full_pipeline(\n",
    "        input_filepath='cardio_train.csv',\n",
    "        output_filepath='cardio_train_mlp_pred.csv',\n",
    "        drop_cols=['id'] \n",
    "    )\n",
    "    \n",
    "    # CNN Example\n",
    "    print(\"\\nRunning Tabular CNN Pipeline...\")\n",
    "    cnn_clf = CardioTabularCNNClassifier(\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        filters=[32, 64],\n",
    "        kernel_sizes=[3, 3],\n",
    "        lr=0.001,\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        scale_features=True\n",
    "    )\n",
    "    \n",
    "    cnn_metrics = cnn_clf.run_full_pipeline(\n",
    "        input_filepath='cardio_train.csv',\n",
    "        output_filepath='cardio_train_cnn_pred.csv',\n",
    "        drop_cols=['id'] \n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
